{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XShk0X9jE24p"
      },
      "source": [
        "Based on the code developed by <a href=\"https://github.com/janblechschmidt/PDEsByNNs\" target=\"_parent\"> Jan Blechschmidt </a> (originally introduced by <a href=\"https://github.com/maziarraissi/PINNs\" target=\"_parent\"> Maziar Raissi </a>).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "8kzRCxT2E24s"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs2wxbZ3E24t"
      },
      "source": [
        "The ordinary differential equation is\n",
        "\n",
        "$\\frac{d^2 u}{dx^2} = \\alpha$,\n",
        "\n",
        "applied to the domain of $x \\in [0, 1]$,\n",
        "\n",
        "subject to boundary condition of\n",
        "\n",
        "$u(0) = u(1) = 0$.\n",
        "\n",
        "The aim is to find the solution for $\\alpha = 0.3$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "gG948wgnE24u"
      },
      "outputs": [],
      "source": [
        "# Set data type\n",
        "DTYPE='float32'   # 'float64'\n",
        "tf.keras.backend.set_floatx(DTYPE)\n",
        "\n",
        "# Define boundary condition\n",
        "def fun_u_b(x):\n",
        "    n = x.shape[0]\n",
        "    return tf.zeros((n,1), dtype=DTYPE)\n",
        "\n",
        "alpha_test = 0.3\n",
        "\n",
        "def heat_eq_exact_solution(x, alpha):\n",
        "    return alpha/2.0*x*(x-1)\n",
        "\n",
        "# Define residual of the DE (we keep x, u, and u_x in the input for the generalization purpose)\n",
        "def fun_r(x, u, u_x, u_xx, alpha):\n",
        "    return alpha - u_xx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "RIdDd9IHE24w"
      },
      "outputs": [],
      "source": [
        "# Set number of data points\n",
        "N_b = 20 # Number of data point for imposing the boundary condition (Dirichlet)\n",
        "N_r = 500 # Number of data point for minimizing the residual of the PDE\n",
        "\n",
        "# Set boundary\n",
        "xmin = 0.\n",
        "xmax = 1.\n",
        "\n",
        "# Lower bounds\n",
        "lb = tf.constant(xmin, dtype=DTYPE)\n",
        "# Upper bounds\n",
        "ub = tf.constant(xmax, dtype=DTYPE)\n",
        "\n",
        "# Set random seed for reproducible results\n",
        "tf.random.set_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "jfrnrfe3E24w"
      },
      "outputs": [],
      "source": [
        "# Boundary data\n",
        "X_b = lb + (ub - lb) * tf.keras.backend.random_bernoulli((N_b,1), 0.5, dtype=DTYPE)\n",
        "\n",
        "# Evaluate boundary condition at (t_b,x_b)\n",
        "u_b = fun_u_b(X_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "DATxhjCxE24z"
      },
      "outputs": [],
      "source": [
        "# Draw uniformly sampled collocation points\n",
        "X_r = tf.random.uniform((N_r,1), lb, ub, dtype=DTYPE)\n",
        "\n",
        "# Collect boundary and inital data in lists\n",
        "X_data = [X_b]\n",
        "u_data = [u_b]\n",
        "\n",
        "# Copy original data\n",
        "X_param = X_data\n",
        "u_param = u_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "oaQVpHsmE240"
      },
      "outputs": [],
      "source": [
        "#N_d = 500   # Number of provided data points\n",
        "#noise = 0.01\n",
        "\n",
        "# Draw points with measurements randomly\n",
        "#X_d = tf.random.uniform((N_d,1), lb, ub, dtype=DTYPE)\n",
        "#u_d = heat_eq_exact_solution(X_d, alpha_test)\n",
        "#u_d += noise * tf.random.normal(u_d.shape, dtype=DTYPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "VtfTk6Y0E241",
        "outputId": "06aef59f-53a5-4396-ddf0-c11f1508489e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(9,6))\n",
        "plt.scatter(X_b, tf.zeros((X_b.shape[0],1)), c=u_b, marker='X', vmin=-1, vmax=1)\n",
        "plt.scatter(X_r, tf.zeros((X_r.shape[0],1)), c='r', marker='.', alpha=0.1)\n",
        "#plt.scatter(X_d, tf.zeros((X_d.shape[0],1)), c=u_d, marker='X', vmin=-1, vmax=1)\n",
        "plt.xlabel('$x$')\n",
        "\n",
        "plt.title('Positions of collocation points');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "AJOcpWgmE242"
      },
      "outputs": [],
      "source": [
        "# The exact data will be treated similar to the IC and Dirichlet BC\n",
        "#X_param.append(X_d)\n",
        "#u_param.append(u_d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "6ycvfHMhE243"
      },
      "outputs": [],
      "source": [
        "# Creating a class derived from Keras Model to modify it according to the needed architecture\n",
        "class PINN_NeuralNet(tf.keras.Model):\n",
        "    \"\"\" Set basic architecture of the PINN model.\"\"\"\n",
        "\n",
        "    def __init__(self, lb, ub,\n",
        "            output_dim=1,\n",
        "            num_hidden_layers=4,\n",
        "            num_neurons_per_layer=10,\n",
        "            activation='tanh',\n",
        "            kernel_initializer='glorot_normal',\n",
        "            **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.output_dim = output_dim\n",
        "        self.lb = lb\n",
        "        self.ub = ub\n",
        "\n",
        "        # Define NN architecture\n",
        "        self.scale = tf.keras.layers.Lambda(\n",
        "            lambda x: 2.0*(x - lb)/(ub - lb) - 1.0)\n",
        "        self.hidden = [tf.keras.layers.Dense(num_neurons_per_layer,\n",
        "                             activation=tf.keras.activations.get(activation),\n",
        "                             kernel_initializer=kernel_initializer)\n",
        "                           for _ in range(self.num_hidden_layers)]\n",
        "        self.out = tf.keras.layers.Dense(output_dim)\n",
        "\n",
        "        # Initialize variable for unknown prameter (alpha)\n",
        "        self.alpha = alpha_test # tf.Variable(1.0, trainable=True, dtype=DTYPE) # We use the same data type for alpha (as the other variables)\n",
        "\n",
        "    def call(self, X):\n",
        "        \"\"\"Forward-pass through neural network.\"\"\"\n",
        "        Z = self.scale(X)\n",
        "        for i in range(self.num_hidden_layers):\n",
        "            Z = self.hidden[i](Z)\n",
        "        return self.out(Z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "rJCFCZtkE243"
      },
      "outputs": [],
      "source": [
        "def get_r(model, X_r):\n",
        "\n",
        "    # A tf.GradientTape is used to compute derivatives in TensorFlow\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        x = X_r #[:, 0:1]\n",
        "\n",
        "        # Variables t and x are watched during tape. Keeping \"watch_accessed_variables=True\" (default) still we cannot can skip this step\n",
        "        # to compute derivatives\n",
        "        tape.watch(x)\n",
        "\n",
        "        # Determine residual\n",
        "        u = model(x)#tf.stack(x[:,0]))\n",
        "\n",
        "        # Compute gradient u_x within the GradientTape\n",
        "        u_x = tape.gradient(u, x) # since we also need the derivative of u_x (du_x/dx), it should be inside the tape\n",
        "\n",
        "    u_xx = tape.gradient(u_x, x) # For higher efficiency these gradients are calculated outside the Tape.\n",
        "\n",
        "    del tape\n",
        "\n",
        "    return fun_r(x, u, u_x, u_xx, model.alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "XciT4Q1IE244"
      },
      "outputs": [],
      "source": [
        "def compute_loss(model, X_r, X_data, u_data):\n",
        "\n",
        "    # Compute phi^r\n",
        "    r = get_r(model, X_r)\n",
        "    phi_r = tf.reduce_mean(tf.square(r))\n",
        "\n",
        "    # Initialize loss\n",
        "    pde_factor = 1.0  # This is used to make its dimension consistent with the loss (in case for example Dx is far from unity)\n",
        "    loss = pde_factor*phi_r\n",
        "\n",
        "    # Add phi^0 and phi^b to the loss\n",
        "    for i in range(len(X_data)):\n",
        "        u_pred = model(X_data[i])\n",
        "        loss += tf.reduce_mean(tf.square(u_data[i] - u_pred))\n",
        "\n",
        "    return loss, phi_r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "0_Sr1GWsE244"
      },
      "outputs": [],
      "source": [
        "def get_grad(model, X_r, X_data, u_data):\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        # This tape is for derivatives with\n",
        "        # respect to trainable variables\n",
        "        tape.watch(model.trainable_variables)\n",
        "        loss, loss_pde = compute_loss(model, X_r, X_data, u_data)\n",
        "\n",
        "    grad = tape.gradient(loss, model.trainable_variables)\n",
        "    del tape\n",
        "\n",
        "    return loss, loss_pde, grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "OaBOG0SME245"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = PINN_NeuralNet(lb, ub, num_hidden_layers=2,\n",
        "                            activation= 'tanh', # tf.keras.layers.LeakyReLU(alpha=0.1)\n",
        "                            kernel_initializer='glorot_normal') # 'he_normal'\n",
        "\n",
        "#model.build(input_shape=(None,1))\n",
        "\n",
        "# We choose a piecewise decay of the learning rate, i.e., the\n",
        "# step size in the gradient descent type algorithm\n",
        "# the first 1000 steps use a learning rate of 0.01\n",
        "# from 1000 - 3000: learning rate = 0.001\n",
        "# from 3000 onwards: learning rate = 0.0005\n",
        "\n",
        "learn_rate = tf.keras.optimizers.schedules.PiecewiseConstantDecay([1000,3000,10000],[2e-3,1e-3,5e-4,1e-4])\n",
        "\n",
        "# Choose the optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Ve3OM2TjE245"
      },
      "outputs": [],
      "source": [
        "# Define one training step as a TensorFlow function to increase speed of training\n",
        "@tf.function\n",
        "def train_step():\n",
        "    # Compute current loss and gradient w.r.t. parameters\n",
        "    loss, loss_pde, grad_theta = get_grad(model, X_r, X_param, u_param)\n",
        "\n",
        "    # Perform gradient descent step\n",
        "    optimizer.apply_gradients(zip(grad_theta, model.trainable_variables))\n",
        "\n",
        "    return loss, loss_pde"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0j4xUK3OE246",
        "outputId": "39b9a29a-da9c-48b8-a30b-c54ea3ec18e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It 00000: loss = 3.66329908e-01 loss_pde = 3.64820093e-01\n",
            "It 00050: loss = 2.40769088e-02 loss_pde = 2.10391805e-02\n",
            "It 00100: loss = 1.50331752e-02 loss_pde = 1.48677509e-02\n",
            "It 00150: loss = 1.03441738e-02 loss_pde = 1.02149369e-02\n",
            "It 00200: loss = 7.00442120e-03 loss_pde = 6.94644684e-03\n",
            "It 00250: loss = 4.67101950e-03 loss_pde = 4.64965729e-03\n",
            "It 00300: loss = 3.04896198e-03 loss_pde = 3.04334098e-03\n",
            "It 00350: loss = 1.94469385e-03 loss_pde = 1.94386404e-03\n",
            "It 00400: loss = 1.19475683e-03 loss_pde = 1.19467953e-03\n",
            "It 00450: loss = 6.76576514e-04 loss_pde = 6.76293625e-04\n",
            "It 00500: loss = 3.28156486e-04 loss_pde = 3.27671616e-04\n",
            "It 00550: loss = 1.30265224e-04 loss_pde = 1.29870881e-04\n",
            "It 00600: loss = 5.25173782e-05 loss_pde = 5.23244380e-05\n",
            "It 00650: loss = 3.27654307e-05 loss_pde = 3.26814115e-05\n",
            "It 00700: loss = 2.75072471e-05 loss_pde = 2.74610757e-05\n",
            "It 00750: loss = 2.44806397e-05 loss_pde = 2.44482708e-05\n",
            "It 00800: loss = 2.19801568e-05 loss_pde = 2.19548128e-05\n",
            "It 00850: loss = 1.98402540e-05 loss_pde = 1.98198595e-05\n",
            "It 00900: loss = 1.80211136e-05 loss_pde = 1.80046918e-05\n",
            "It 00950: loss = 1.64892499e-05 loss_pde = 1.64761204e-05\n",
            "It 01000: loss = 1.52120001e-05 loss_pde = 1.52016337e-05\n",
            "It 01050: loss = 1.46481188e-05 loss_pde = 1.46389502e-05\n",
            "It 01100: loss = 1.41396804e-05 loss_pde = 1.41315868e-05\n",
            "It 01150: loss = 1.36729504e-05 loss_pde = 1.36658455e-05\n",
            "It 01200: loss = 1.32467912e-05 loss_pde = 1.32405949e-05\n",
            "It 01250: loss = 1.28594784e-05 loss_pde = 1.28540896e-05\n",
            "It 01300: loss = 1.25091274e-05 loss_pde = 1.25044671e-05\n",
            "It 01350: loss = 1.21933917e-05 loss_pde = 1.21894009e-05\n",
            "It 01400: loss = 1.19100514e-05 loss_pde = 1.19066362e-05\n",
            "It 01450: loss = 1.16563588e-05 loss_pde = 1.16534320e-05\n",
            "It 01500: loss = 1.14296372e-05 loss_pde = 1.14271543e-05\n",
            "It 01550: loss = 1.12272483e-05 loss_pde = 1.12251591e-05\n",
            "It 01600: loss = 1.10465189e-05 loss_pde = 1.10447554e-05\n",
            "It 01650: loss = 1.08848290e-05 loss_pde = 1.08833501e-05\n",
            "It 01700: loss = 1.07397072e-05 loss_pde = 1.07384776e-05\n",
            "It 01750: loss = 1.06088282e-05 loss_pde = 1.06078014e-05\n",
            "It 01800: loss = 1.04901110e-05 loss_pde = 1.04892579e-05\n",
            "It 01850: loss = 1.03814164e-05 loss_pde = 1.03807015e-05\n",
            "It 01900: loss = 1.02811609e-05 loss_pde = 1.02805698e-05\n",
            "It 01950: loss = 1.01876594e-05 loss_pde = 1.01871719e-05\n",
            "It 02000: loss = 1.00996031e-05 loss_pde = 1.00991929e-05\n",
            "It 02050: loss = 1.00158923e-05 loss_pde = 1.00155466e-05\n",
            "It 02100: loss = 9.93555113e-06 loss_pde = 9.93526555e-06\n",
            "It 02150: loss = 9.85758197e-06 loss_pde = 9.85733914e-06\n",
            "It 02200: loss = 9.78145181e-06 loss_pde = 9.78124808e-06\n",
            "It 02250: loss = 9.70664314e-06 loss_pde = 9.70646488e-06\n",
            "It 02300: loss = 9.63254570e-06 loss_pde = 9.63239654e-06\n",
            "It 02350: loss = 9.55888936e-06 loss_pde = 9.55875657e-06\n",
            "It 02400: loss = 9.48541037e-06 loss_pde = 9.48529487e-06\n",
            "It 02450: loss = 9.41192047e-06 loss_pde = 9.41181952e-06\n",
            "It 02500: loss = 9.33822594e-06 loss_pde = 9.33813953e-06\n",
            "It 02550: loss = 9.26419125e-06 loss_pde = 9.26411303e-06\n",
            "It 02600: loss = 9.18969181e-06 loss_pde = 9.18961996e-06\n",
            "It 02650: loss = 9.11478855e-06 loss_pde = 9.11472125e-06\n",
            "It 02700: loss = 9.03935597e-06 loss_pde = 9.03929413e-06\n",
            "It 02750: loss = 8.96339952e-06 loss_pde = 8.96334313e-06\n",
            "It 02800: loss = 8.88696104e-06 loss_pde = 8.88690647e-06\n",
            "It 02850: loss = 8.80999596e-06 loss_pde = 8.80994412e-06\n",
            "It 02900: loss = 8.73250519e-06 loss_pde = 8.73245790e-06\n",
            "It 02950: loss = 8.65449874e-06 loss_pde = 8.65445418e-06\n",
            "It 03000: loss = 8.57598661e-06 loss_pde = 8.57594296e-06\n",
            "It 03050: loss = 8.53556958e-06 loss_pde = 8.53552774e-06\n",
            "It 03100: loss = 8.49526259e-06 loss_pde = 8.49522075e-06\n",
            "It 03150: loss = 8.45436807e-06 loss_pde = 8.45432896e-06\n",
            "It 03200: loss = 8.41279507e-06 loss_pde = 8.41275687e-06\n",
            "It 03250: loss = 8.37063817e-06 loss_pde = 8.37059906e-06\n",
            "It 03300: loss = 8.32783553e-06 loss_pde = 8.32779733e-06\n",
            "It 03350: loss = 8.28441989e-06 loss_pde = 8.28438260e-06\n",
            "It 03400: loss = 8.24036215e-06 loss_pde = 8.24032668e-06\n",
            "It 03450: loss = 8.19567504e-06 loss_pde = 8.19563866e-06\n",
            "It 03500: loss = 8.15037765e-06 loss_pde = 8.15034218e-06\n",
            "It 03550: loss = 8.10441816e-06 loss_pde = 8.10438451e-06\n",
            "It 03600: loss = 8.05787295e-06 loss_pde = 8.05784020e-06\n",
            "It 03650: loss = 8.01072565e-06 loss_pde = 8.01069382e-06\n",
            "It 03700: loss = 7.96297081e-06 loss_pde = 7.96293989e-06\n",
            "It 03750: loss = 7.91461753e-06 loss_pde = 7.91458569e-06\n",
            "It 03800: loss = 7.86561213e-06 loss_pde = 7.86558303e-06\n",
            "It 03850: loss = 7.81607650e-06 loss_pde = 7.81604740e-06\n",
            "It 03900: loss = 7.76592879e-06 loss_pde = 7.76590059e-06\n",
            "It 03950: loss = 7.71516716e-06 loss_pde = 7.71513987e-06\n",
            "It 04000: loss = 7.66386256e-06 loss_pde = 7.66383346e-06\n",
            "It 04050: loss = 7.61199726e-06 loss_pde = 7.61196998e-06\n",
            "It 04100: loss = 7.55956489e-06 loss_pde = 7.55953852e-06\n",
            "It 04150: loss = 7.50656682e-06 loss_pde = 7.50653953e-06\n",
            "It 04200: loss = 7.45304624e-06 loss_pde = 7.45302123e-06\n",
            "It 04250: loss = 7.39897314e-06 loss_pde = 7.39894813e-06\n",
            "It 04300: loss = 7.34435935e-06 loss_pde = 7.34433434e-06\n",
            "It 04350: loss = 7.28926307e-06 loss_pde = 7.28923987e-06\n",
            "It 04400: loss = 7.23363155e-06 loss_pde = 7.23360745e-06\n",
            "It 04450: loss = 7.17747116e-06 loss_pde = 7.17744842e-06\n",
            "It 04500: loss = 7.12085102e-06 loss_pde = 7.12082965e-06\n",
            "It 04550: loss = 7.06376068e-06 loss_pde = 7.06373839e-06\n",
            "It 04600: loss = 7.00616556e-06 loss_pde = 7.00614373e-06\n",
            "It 04650: loss = 6.94815571e-06 loss_pde = 6.94813207e-06\n",
            "It 04700: loss = 6.88967839e-06 loss_pde = 6.88965838e-06\n",
            "It 04750: loss = 6.83076860e-06 loss_pde = 6.83074813e-06\n",
            "It 04800: loss = 6.77139724e-06 loss_pde = 6.77137768e-06\n",
            "It 04850: loss = 6.71166435e-06 loss_pde = 6.71164617e-06\n",
            "It 04900: loss = 6.65151492e-06 loss_pde = 6.65149537e-06\n",
            "It 04950: loss = 6.59101170e-06 loss_pde = 6.59099305e-06\n",
            "It 05000: loss = 6.53008965e-06 loss_pde = 6.53007191e-06\n",
            "It 05050: loss = 6.46880335e-06 loss_pde = 6.46878652e-06\n",
            "It 05100: loss = 6.40717826e-06 loss_pde = 6.40716098e-06\n",
            "It 05150: loss = 6.34524076e-06 loss_pde = 6.34522894e-06\n",
            "It 05200: loss = 6.28298585e-06 loss_pde = 6.28295447e-06\n",
            "It 05250: loss = 6.22074140e-06 loss_pde = 6.22068637e-06\n",
            "It 05300: loss = 6.15776662e-06 loss_pde = 6.15776071e-06\n",
            "It 05350: loss = 6.10543975e-06 loss_pde = 6.09996277e-06\n",
            "It 05400: loss = 6.03180797e-06 loss_pde = 6.03172748e-06\n",
            "It 05450: loss = 5.96850396e-06 loss_pde = 5.96849304e-06\n",
            "It 05500: loss = 5.91265780e-06 loss_pde = 5.90947957e-06\n",
            "It 05550: loss = 5.84162262e-06 loss_pde = 5.84161717e-06\n",
            "It 05600: loss = 5.77795390e-06 loss_pde = 5.77794435e-06\n",
            "It 05650: loss = 5.71900591e-06 loss_pde = 5.71626424e-06\n",
            "It 05700: loss = 5.65047640e-06 loss_pde = 5.65036453e-06\n",
            "It 05750: loss = 5.58645343e-06 loss_pde = 5.58644433e-06\n",
            "It 05800: loss = 5.52433312e-06 loss_pde = 5.52317715e-06\n",
            "It 05850: loss = 5.45866260e-06 loss_pde = 5.45857483e-06\n",
            "It 05900: loss = 5.39457642e-06 loss_pde = 5.39456141e-06\n",
            "It 05950: loss = 5.33636239e-06 loss_pde = 5.33297953e-06\n",
            "It 06000: loss = 5.26673739e-06 loss_pde = 5.26671738e-06\n",
            "It 06050: loss = 5.20265758e-06 loss_pde = 5.20263302e-06\n",
            "It 06100: loss = 5.14114163e-06 loss_pde = 5.14008343e-06\n",
            "It 06150: loss = 5.07522782e-06 loss_pde = 5.07522327e-06\n",
            "It 06200: loss = 5.01149225e-06 loss_pde = 5.01148361e-06\n",
            "It 06250: loss = 4.97796191e-06 loss_pde = 4.96170333e-06\n",
            "It 06300: loss = 4.88483329e-06 loss_pde = 4.88476962e-06\n",
            "It 06350: loss = 4.82174255e-06 loss_pde = 4.82172845e-06\n",
            "It 06400: loss = 4.75886509e-06 loss_pde = 4.75871093e-06\n",
            "It 06450: loss = 4.69883116e-06 loss_pde = 4.69697670e-06\n",
            "It 06500: loss = 4.63394235e-06 loss_pde = 4.63394144e-06\n",
            "It 06550: loss = 4.57182250e-06 loss_pde = 4.57181523e-06\n",
            "It 06600: loss = 4.52304675e-06 loss_pde = 4.51451160e-06\n",
            "It 06650: loss = 4.44921670e-06 loss_pde = 4.44911848e-06\n",
            "It 06700: loss = 4.38813686e-06 loss_pde = 4.38811958e-06\n",
            "It 06750: loss = 4.54979818e-06 loss_pde = 4.40371196e-06\n",
            "It 06800: loss = 4.26951328e-06 loss_pde = 4.26916131e-06\n",
            "It 06850: loss = 4.21021650e-06 loss_pde = 4.21020104e-06\n",
            "It 06900: loss = 4.15166323e-06 loss_pde = 4.15165550e-06\n",
            "It 06950: loss = 4.09321638e-06 loss_pde = 4.09320910e-06\n",
            "It 07000: loss = 4.14848955e-06 loss_pde = 4.07070365e-06\n",
            "It 07050: loss = 3.98051270e-06 loss_pde = 3.98022030e-06\n",
            "It 07100: loss = 3.92447646e-06 loss_pde = 3.92445827e-06\n",
            "It 07150: loss = 3.86905731e-06 loss_pde = 3.86905140e-06\n",
            "It 07200: loss = 3.81388190e-06 loss_pde = 3.81383506e-06\n",
            "It 07250: loss = 3.76129969e-06 loss_pde = 3.76033222e-06\n",
            "It 07300: loss = 3.70691077e-06 loss_pde = 3.70681573e-06\n",
            "It 07350: loss = 3.65377218e-06 loss_pde = 3.65376877e-06\n",
            "It 07400: loss = 4.18889340e-06 loss_pde = 3.78013397e-06\n",
            "It 07450: loss = 3.55467978e-06 loss_pde = 3.55151724e-06\n",
            "It 07500: loss = 3.50012238e-06 loss_pde = 3.50008600e-06\n",
            "It 07550: loss = 3.44979730e-06 loss_pde = 3.44979435e-06\n",
            "It 07600: loss = 3.55741463e-06 loss_pde = 3.44731939e-06\n",
            "It 07650: loss = 3.35265554e-06 loss_pde = 3.35264826e-06\n",
            "It 07700: loss = 3.30501985e-06 loss_pde = 3.30500984e-06\n",
            "It 07750: loss = 3.25744304e-06 loss_pde = 3.25743963e-06\n",
            "It 07800: loss = 3.33926278e-06 loss_pde = 3.24830148e-06\n",
            "It 07850: loss = 3.16709270e-06 loss_pde = 3.16642831e-06\n",
            "It 07900: loss = 3.12123643e-06 loss_pde = 3.12123098e-06\n",
            "It 07950: loss = 3.07647520e-06 loss_pde = 3.07646906e-06\n",
            "It 08000: loss = 3.10186647e-06 loss_pde = 3.05105709e-06\n",
            "It 08050: loss = 2.99010298e-06 loss_pde = 2.98997793e-06\n",
            "It 08100: loss = 2.94761753e-06 loss_pde = 2.94760753e-06\n",
            "It 08150: loss = 2.90646312e-06 loss_pde = 2.90566913e-06\n",
            "It 08200: loss = 2.89581953e-06 loss_pde = 2.87330704e-06\n",
            "It 08250: loss = 2.82415158e-06 loss_pde = 2.82401174e-06\n",
            "It 08300: loss = 2.78359403e-06 loss_pde = 2.78358721e-06\n",
            "It 08350: loss = 2.93453377e-06 loss_pde = 2.79674964e-06\n",
            "It 08400: loss = 2.70711234e-06 loss_pde = 2.70614169e-06\n",
            "It 08450: loss = 2.66774668e-06 loss_pde = 2.66774191e-06\n",
            "It 08500: loss = 2.62980780e-06 loss_pde = 2.62968479e-06\n",
            "It 08550: loss = 2.66077018e-06 loss_pde = 2.61156833e-06\n",
            "It 08600: loss = 2.55703321e-06 loss_pde = 2.55691930e-06\n",
            "It 08650: loss = 2.52097402e-06 loss_pde = 2.52096265e-06\n",
            "It 08700: loss = 2.48520291e-06 loss_pde = 2.48510082e-06\n",
            "It 08750: loss = 2.52724135e-06 loss_pde = 2.47149023e-06\n",
            "It 08800: loss = 2.41669727e-06 loss_pde = 2.41667271e-06\n",
            "It 08850: loss = 2.38286043e-06 loss_pde = 2.38281677e-06\n",
            "It 08900: loss = 2.34910067e-06 loss_pde = 2.34903450e-06\n",
            "It 08950: loss = 2.31808326e-06 loss_pde = 2.31686931e-06\n",
            "It 09000: loss = 2.28505996e-06 loss_pde = 2.28457952e-06\n",
            "It 09050: loss = 2.25253575e-06 loss_pde = 2.25252666e-06\n",
            "It 09100: loss = 2.22058520e-06 loss_pde = 2.22057838e-06\n",
            "It 09150: loss = 2.45679553e-06 loss_pde = 2.25943995e-06\n",
            "It 09200: loss = 2.16180638e-06 loss_pde = 2.16080798e-06\n",
            "It 09250: loss = 2.13073781e-06 loss_pde = 2.13072985e-06\n",
            "It 09300: loss = 2.10107669e-06 loss_pde = 2.10106759e-06\n",
            "It 09350: loss = 2.07166772e-06 loss_pde = 2.07143285e-06\n",
            "It 09400: loss = 2.04569733e-06 loss_pde = 2.04371622e-06\n",
            "It 09450: loss = 2.01513217e-06 loss_pde = 2.01485227e-06\n",
            "It 09500: loss = 1.98668977e-06 loss_pde = 1.98667863e-06\n",
            "It 09550: loss = 1.95927191e-06 loss_pde = 1.95874804e-06\n",
            "It 09600: loss = 1.94413110e-06 loss_pde = 1.93519918e-06\n"
          ]
        }
      ],
      "source": [
        "# Number of training epochs\n",
        "N = 20000\n",
        "loss_history = []\n",
        "loss_pde_history = []\n",
        "#alpha_list = []\n",
        "\n",
        "from time import time\n",
        "\n",
        "# Start timer\n",
        "t0 = time()\n",
        "\n",
        "for i in range(N+1):\n",
        "\n",
        "    loss, loss_pde = train_step()\n",
        "\n",
        "    # Append current loss to hist\n",
        "    loss_history.append(loss.numpy())\n",
        "    loss_pde_history.append(loss_pde.numpy())\n",
        "\n",
        "    #alpha = model.alpha.numpy()\n",
        "    #alpha_list.append(alpha)\n",
        "\n",
        "    # Output current loss after 50 iterates\n",
        "    if i%50 == 0:\n",
        "        print('It {:05d}: loss = {:10.8e} loss_pde = {:10.8e}'.format(i,loss,loss_pde))\n",
        "        #print('It {:05d}: loss = {:10.8e} loss_pde = {:10.8e} alpha = {:10.8e}'.format(i,loss,loss_pde,alpha))\n",
        "\n",
        "#alpha_rel_error = np.abs((model.alpha.numpy()-alpha_test)/alpha_test)\n",
        "#print('Relative error of alpha ', alpha_rel_error)\n",
        "\n",
        "# Print computation time\n",
        "print('\\nComputation time: {} seconds'.format(time()-t0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQh3aJXKE246"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(9,6))\n",
        "ax1 = fig.add_subplot(111)\n",
        "ax1.semilogy(range(len(loss_history)), loss_history,'k-')\n",
        "ax1.semilogy(range(len(loss_history)), loss_pde_history,'b-')\n",
        "ax1.set_xlabel('$n_{epoch}$', fontsize = 18)\n",
        "ax1.set_ylabel('$\\\\phi_{n_{epoch}}$', fontsize = 18)\n",
        "\n",
        "#ax2 = ax1.twinx()  # instantiate the second axis sharing the same x-axis\n",
        "#color = 'tab:red'\n",
        "#ax2.tick_params(axis='y', labelcolor=color)\n",
        "#ax2.plot(range(len(loss_history)), alpha_list,'-',color=color)\n",
        "#ax2.set_ylabel('$\\\\alpha^{n_{epoch}}$', color=color, fontsize = 18)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(10)\n",
        "x_test = tf.random.uniform((50,1), lb, ub, dtype=DTYPE)"
      ],
      "metadata": {
        "id": "y98Vyl5H7h50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "u_modeled = model(x_test).numpy()\n",
        "u_exact = heat_eq_exact_solution(x_test, alpha_test).numpy()\n",
        "error = np.abs((u_modeled - u_exact)/u_exact)"
      ],
      "metadata": {
        "id": "6hxeVdad7xf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(x_test)):\n",
        "    print(x_test[i].numpy(),u_modeled[i],u_exact[i],error[i])"
      ],
      "metadata": {
        "id": "yp5vTi9G8f-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig2 = plt.figure(figsize=(9,6))\n",
        "ax1 = fig2.add_subplot(111)\n",
        "ax1.scatter(x_test.numpy(), u_modeled, c='b', marker='X',s=200, alpha=0.5)\n",
        "ax1.scatter(x_test.numpy(), u_exact, c='r', marker='.', s=100, alpha=1.0)\n",
        "ax1.set_xlabel('$x$', fontsize = 18)\n",
        "ax1.set_ylabel('$u$', fontsize = 18)"
      ],
      "metadata": {
        "id": "I3Q2uQKHH-rI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "ODE_CSUMS2022.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
